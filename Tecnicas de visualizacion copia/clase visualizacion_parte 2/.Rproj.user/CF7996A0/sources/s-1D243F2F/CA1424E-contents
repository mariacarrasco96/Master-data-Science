---
title: "GAM_BIKES"
author: "Carlota Echevarria"
date: "22/10/2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



##Librerias##

```{r}
library(here)
library(readr)
library(tidyverse)
library(skimr) # Beautiful Summarize
library(magrittr) # Pipe operators
library(corrplot) # Correlations
library(ggcorrplot)  # Correlations
library(PerformanceAnalytics) # Correlations
library(leaps) # Model selection
library(caret) # Cross Validation
library(bestglm) # Cross Validation
library(glmnet) # Regularization
library(knitr)
library(ISLR)
library(gam)
library(fBasics)
library(nortest)
library(MASS)
library(rsample)
```

```{r}
set.seed(123)
```


```{r}
day <- read_csv("C:/Users/echev/Desktop/Programación R_Ejercicios/GAM_BIKES/day.csv")
View(day)
```

```{r}
skim(day)
```
 No hay datos repetidos, ni NA.
 
```{r}
eliminar<-c("yr","mnth","dteday","instant") #excluyo variables

#Correlacion
corrplot(cor(day%>%
               select_at(vars(-eliminar)),
            use = "complete.obs" ),
         method ="number", type="upper")

```

```{r}
ggcorrplot(cor(day %>% 
               select_at(vars(-eliminar)), 
            use = "complete.obs"),
            hc.order = TRUE,
            type = "lower",  lab = TRUE)
```

```{r}
chart.Correlation(day %>%  #las lineas rectas rojas, indican que no hay correlacion
               select_at(vars(-eliminar)),
               histogram=TRUE, pch=19)
```

#Seleccion del Modelo 
cnt es el número de bicis alquiladas, y es el resultado de la suma de los casuales y los registrados

```{r}

#Modelo de regresion multiple, habria que estudiar multicolinealidad, normalidad
day1 <- day %>% select_at(vars(-eliminar)) #elimina las tres variables factor


model_select <- regsubsets(cnt~. , data =day1, method = "seqrep",nvmax=24) #si no ponemos 24, solo coge 8.

model_select_summary <- summary(model_select)

data.frame(
  Adj.R2 = (model_select_summary$adjr2),
  CP = (model_select_summary$cp),
  BIC = (model_select_summary$bic)
)
view(data.frame(
  Adj.R2 = (model_select_summary$adjr2),
  CP = (model_select_summary$cp),
  BIC = (model_select_summary$bic)
))


```



```{r}
plot(model_select, scale = "adjr2", main = "Adjusted R^2") #los cuadrados me dicen que en ese modelo que variables estam, las variables que mejor se ajustan al modelo

data.frame(
  Adj.R2 = which.max(model_select_summary$adjr2),
  CP = which.min(model_select_summary$cp),
  BIC = which.min(model_select_summary$bic)
)
```

```{r}
data.frame(
  Adj.R2 = which.max(model_select_summary$adjr2),
  CP = which.min(model_select_summary$cp),
  BIC = which.min(model_select_summary$bic)
)
```

##MODELO GAM

```{r}
#We can also fit a smoothing spline with none other than smooth.spline(). Here we fit a spline with 16 degrees of freedom, then spline chosen by cross-validation, which yields 6.8 degrees of freedom.

#Solo sacaremos los grados de libertad de las variables númericas, las variables categoricas y dumbies no es lógico sacar los grados de libertad


dftemp<-smooth.spline(day$temp, day$cnt, cv=TRUE)
dfatemp<-smooth.spline(day$atemp, day$cnt, cv=TRUE)
dfhum<-smooth.spline(day$hum, day$cnt, cv=TRUE)
dfhum2<-smooth.spline(day$hum, day$cnt, df= 16)
dfwindspeed<-smooth.spline(day$windspeed, day$cnt, cv=TRUE)
dfcasual<-smooth.spline(day$casual, day$cnt, cv=TRUE)
dfregistered<-smooth.spline(day$registered, day$cnt, cv=TRUE)






dftemp$df
dfatemp$df
dfhum$df
dfwindspeed$df
dfcasual$df
dfregistered$df


```
```{r}

```


#Grafico ejemplo de los distintos grados de libertad

```{r}
plot(day$hum, day$cnt, xlim=day$humLims, col='gray')
title('Smoothing Spline')
dfhum<- smooth.spline(day$hum, day$cnt, cv=TRUE)
dfhum2<-smooth.spline(day$hum, day$cnt, df=16)
lines(dfhum, col='red', lwd=2)
lines(dfhum2, col='blue', lwd=1)
legend('topright', legend=c('4.548 DF', '16 DF'),
       col=c('red','blue'), lty=1, lwd=2, cex=0.8)
```


Transformación de las variables categoricas

```{r}
day$weekday <- as.factor(day$weekday)
day$weathersit<- as.factor(day$weathersit)
day$season <- as.factor(day$season)

skim(day)
```

Grados de libertad de las variables númericas
```{r}
dftemp$df
dfatemp$df
dfhum$df
dfwindspeed$df
dfcasual$df
dfregistered$df
```



###Modeo GAM###
Todas las variables
```{r}
gam1 <- gam(cnt~s(temp, df =9.1) + s(atemp, df=8.8) + s(hum, df=4.54) + s(windspeed, df=6.00) + s(casual, df=11.27) + s(registered, df=12.95) + holiday + weekday + workingday + weathersit + season,data=day)

plot(gam1, se=TRUE, col='red')
```


```{r}
summary(gam1)
```

Nuevo modelo sin holiday ni season

```{r}

gam2 <- gam(cnt~s(temp, df =9.1) + s(atemp, df=8.8) + s(hum, df=4.54) + s(windspeed, df=6.00) + s(casual, df=11.27) + s(registered, df=12.95) +  weekday + workingday + weathersit,data=day)

plot(gam2, se=TRUE, col='red')
```

```{r}
summary(gam2)
```

Elimino Workingday

```{r}
gam3 <- gam(cnt~s(temp, df =9.1) + s(atemp, df=8.8) + s(hum, df=4.54) + s(windspeed, df=6.00) + s(casual, df=11.27) + s(registered, df=12.95) +  weekday +  weathersit,data=day)

plot(gam3, se=TRUE, col='red')
```

```{r}
summary(gam3)
```

Elimino Weekday
```{r}

gam4 <- gam(cnt~s(temp, df =9.1) + s(atemp, df=8.8) + s(hum, df=4.54) + s(windspeed, df=6.00) + s(casual, df=11.27) + s(registered, df=12.95) + workingday + weathersit,data=day)


plot(gam4, se=TRUE, col='red')
```

```{r}
summary(gam4)
```

ANNOVA:Comparacion

```{r}
anova(gam1, gam2, gam3, gam4, test='F') #Nos interesa el que menor residuo tenga
```

Segun los residuos vemos cual es el mejor modelo. A continuación procedemos a realizar el CROSS-VALIDATION

```{r}
set.seed(123)
day_split <- initial_split(day, prop = 0.7, strata = "cnt")
day_train <- training(day_split)
day_test  <- testing(day_split)
```

Predicimos el mejor modelo, con el train (70%)
```{r}
gam_train <- gam(cnt~s(temp, df =9.1) + s(atemp, df=8.8) + s(hum, df=4.54) + s(windspeed, df=6.00) + s(casual, df=11.27) + s(registered, df=12.95) +  weekday + workingday + weathersit,data=day_train)

plot(gam_train, se=TRUE, col='red')
```


```{r}
summary(gam_train)
```

Pasamos a predecir con todos los datos #CNT es la suma de registred y casual, de ahi que el error sea tan bajo.
```{r}

pred.modelo.gam <-  predict(gam2, day_test)
test.error.gam <- mean((pred.modelo.gam - day_test$cnt)^2)
test.error.gam

```

Hay que volver a realizar los gam, con todas las variables salvo casual y registered.







